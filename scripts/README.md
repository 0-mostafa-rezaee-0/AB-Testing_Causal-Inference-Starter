# ğŸ“‚ scripts/

This folder contains modular Python scripts for simulation, statistical analysis, and visualization used throughout the A/B testing project.

These scripts help keep the notebook clean and allow for easier reuse, testing, and automation.

---

## ğŸ“œ File Overview

### âœ… `abtest_utils.py`
Core utility functions for statistical testing and simulation:

- `simulate_ab_test()` â€” Generate binary outcome data for control and treatment
- `compute_z_score()` â€” Calculate pooled SE, z-statistic, and p-value manually
- `bootstrap_ci()` â€” Estimate confidence intervals for lift using bootstrapping
- `simulate_false_positives()` â€” Run multiple null experiments and report FPR

### ğŸ“Š `Conversion-Bar-Ci.py`
Generates a bar plot with 95% confidence intervals for control vs. treatment conversion rates.
- Used for visual storytelling in notebook and README
- Output: `images/conversion_bar_ci.png`

### ğŸ“‰ `p-value-distribution.py`
Simulates 1000 null experiments (no treatment effect) and visualizes the p-value distribution.
- Illustrates the base rate of false positives
- Output: `images/pvalue_distribution.png`

---

## ğŸ§ª Usage
These scripts are not meant to be executed directly like a CLI tool but are instead intended to be:
- Imported into Jupyter notebooks
- Used in automated pipelines or tests
- Extended for future projects

---

## ğŸ“¦ Coming Soon
Future scripts may include:
- Sequential testing logic
- Multi-metric analysis
- Integration with CLI or API endpoints

---

## ğŸ§‘â€ğŸ’» Tip for Contributors
If you add a new script, include:
1. A docstring at the top
2. Function docstrings with input/output
3. A line in this README for discoverability

Stay modular. Keep your experiments reproducible. ğŸ§ª